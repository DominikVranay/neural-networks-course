{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cvičenie 11: Hebbovo učenie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pri trénovaní neurónových sietí je vždy kľúčová otázka spôsob aktualizácie váh. Najčastejšie sa na to použije algoritmus backpropagation alebo niektorý z jeho variantov. Algoritmus je síce účinný a zjednodušuje spôsob trénovania neurónových sietí, má ale niekoľko nedostatkov a tým hlavným je to, že váhy sa aktualizujú postupne z posledných vrstiev smerom ku vstupnej vrstve. Moderné výpočtové zdroje umožňujú paralelizovať niektoré výpočty, ale efektivita trénovania stále nepribližuje k teoretickým maximálnym možnostiam.\n",
    "\n",
    "Dnes uvidíme, ako funguje alternatívny spôsob aktualizácie váh neurónov, ktorý je založený na teórii učenia biologických neurónov - Hebbovo učenie. Hebbovo učenie sa aplikuje pri nekontrolovanom učení a jeden z najčastejších prípadov použitia je analýza hlavných komponentov, teda získanie prvotných informácií o datasete. Na dnešnom cvičení Hebbovo učenie použijeme na získanie vlastných vektorov množiny dát."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vlastné hodnoty a vlastné vektory\n",
    "\n",
    "Vlastný vektor (*eigenvector*) je určený pre lineárny operátor, ktorý keď aplikujeme, smer vlastného vektoru sa nemení, mení sa iba jeho veľkosť - vektor teda meníme iba násobkom skalárneho čísla. Vlastná hodnota (*eigenvalue*) je číslo, ktorým potrebujeme násobiť vlastný vektor pri aplikácii operátora.\n",
    "\n",
    "Pre datasety majú vlastné vektory ešte jeden špeciálny význam: sú výstupom analýzy hlavných komponentov. Cieľom takejto analýzy je nájsť množinu kombinácií pôvodných príznakov, ktorá zachováva najväčšie množstvo informácií a zároveň má menej dimenzií ako pôvodná množina príznakov. S vlastnými vektormi to súvisí tak, že vlastné vektory kovariančnej matice popisujú zároveň aj hlavné komponenty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Generovanie datasetu\n",
    "\n",
    "Aby sme poukázali na všeobecnosť Hebbovho učenia, nepoužijeme statický dataset, ale náhodne ho vygenerujeme pomocou knižnice `scikit-learn`. Aj keď generovanie bude náhodné, nastavíme ale niekoľko parametrov aby sme zabezpečili opakovateľnosť našich pokusov.\n",
    "\n",
    "Najprv si naimportujeme všetky potrebné moduly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knižnice `numpy` a `matplotlib` už poznáte, a niektoré prvky sme už použili aj zo `scikit-learn`. Dnes najprv použijeme metódu `make_blobs`, ktorá nám vygeneruje náhodné body na základe istých nastavení, a triedu `StandardScaler`, ktorá slúži na škálovanie údajov tak, že vymaže priemerné hodnoty a upraví príznaky na jednotkovú varianciu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "X, _ = make_blobs(\n",
    "    n_samples=500,\n",
    "    centers=2,\n",
    "    cluster_std=5.0,\n",
    "    random_state=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metóda `make_blobs` nám vygeneruje náhodné body, ktoré zorganizuje do zhlukov. Keďže my ale pracujeme s nekontrolovaným učením, očakávané výstupy (zhluky) nepoužijeme. Pri volaní funkcie nastavíme počet vygenerovaných bodov (`n_samples=500`), počet centier zhlukov (`centers=2`), štandardnú deviáciu jednotlivých zhlukov (`cluster_std=5.0`) a nastavíme aj generovanie náhodných čísel pre generovanie bodov (`random_state=1000`). Podrobný popis týchto parametrov nájdete v [dokumentácii](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Úprava dát\n",
    "\n",
    "Po vygenerovaní datasetu potrebujeme ho ešte upraviť a prvou úlohou je škálovanie údajov pomocou už spomínaného `StandardScaler`. Podrobný popis triedy nájdete znova v [dokumentácii](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_std=False)\n",
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Výpočet vlastnej hodnoty a vlastných vektorov\n",
    "\n",
    "Ďalšou úlohou je výpočet vlastných vektorov, ktoré potom chceme aproximovať pomocou Hebbovho učenia. Pre výpočet týchto vektorov najprv určíme kovariančnú maticu (pomocou knižnice `numpy`) a následne určíme vlastnú hodnotu a vlastný vektor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.cov(Xs.T)\n",
    "eigu, eigv = np.linalg.eig(Q)\n",
    "print(eigu)\n",
    "print(eigv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kód by mal vygenerovať dve hodnoty a dva vektory, pričom vektory sa vypisujú v stĺpcoch. Hebbovo učenie nám pomôže pri získaní toho hlavného komponentu, teda vektoru s vyššou vlastnou hodnotou (v našom prípade je to druhý vektor `[-0.6528286, -0.75750566]`).\n",
    "\n",
    "Vektory vieme vizualizovať aj pomocou `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xs[:, 0], Xs[:, 1])\n",
    "plt.quiver(*[0, 0], *eigv[:, 0], color=['r'], scale=21)\n",
    "plt.quiver(*[0, 0], *eigv[:, 1], color=['b'], scale=21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hebbovo učenie\n",
    "\n",
    "Hebbovo učenie je model učenia biologických neurónov, ktorý vymyslel Donald Hebb v roku 1949. Jeho aplikácia v trénovaní umelých neurónových sietí má jednoznačnú výhodu oproti backpropagation v tom, že aktualizácia váh závisí iba od predsynaptického a postsynaptického neurónu, teda aktualizáciu vieme úplne paralelizovať, nemusíme vypočítať chybové signály pre každý neurón postupne. \n",
    "\n",
    "Hebbovo učenie zavedie aj ďalšie zjednodušenia do modelu neurónu, z ktorých hlavný je zmazanie prahu (vieme ho ale reprezentovať ako váha nultého vstupu):\n",
    "\n",
    "![Model neurónu v Hebbovom učení](https://www.bonaccorso.eu/wp-content/uploads/2017/08/ml_hl_1-300x227.png)\n",
    "\n",
    "Takisto nemáme aktivačnú a výstupnú funkciu, tým pádom výstup sa vypočíta iba ako skalárny súčin dvoch vektorov:\n",
    "\n",
    "$y=\\sum_{i=1}^{n} w_{i} x_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samotné trénovanie je založené na jednoduchom princípe: ak pred- a postsynaptické neuróny sa správajú rovnakým spôsobom (teda ich výstupy majú rovnaké znamienka), zosilníme synapsiu, v opačnom prípade hodnotu váhy znížime (*neurons that fire together, wire together*). Formálne to vieme napísať nasledovne:\n",
    "\n",
    "$\\Delta w_{i} = \\gamma x_{i} y$,\n",
    "\n",
    "kde $\\gamma$ je učiaci parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aktualizačné pravidlo pre implementáciu umelej neurónovej siete sformuloval Erkki Oja:\n",
    "\n",
    "$\\Delta w_{i} = \\gamma y (x_{i} - w_{i}y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Implementácia Hebbovho učenia pre jeden neurón\n",
    "\n",
    "V tomto kroku implementujeme Hebbovo učenie pomocou Ojovho pravidla pre jeden neurón. Použijeme pritom dataset vygenerovaný v predošlom kroku, teda s dvoma príznakmi a jedným výstupom. Váhy neurónu by mali konvergovať ku prvotnému vlastnému vektoru.\n",
    "\n",
    "Najprv váhy nainicializujeme náhodne a nainicializujeme aj premennú pre uchovávanie predošlých hodnôt váh (pre určenie konvergencie):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_oja = np.random.normal(scale=0.25, size=(2, 1))\n",
    "prev_W_oja = np.ones((2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ďalej nastavíme parametre učenia: učiaci parameter a toleranciu, ktorá slúži na určenie miery konvergencie. Trénovanie potom ukončíme ak zmena váh je menšia ako tolerancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "tolerance = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Trénovanie neurónu\n",
    "\n",
    "Počas trénovania použijeme Ojovo pravidlo. Neurčíme počet epoch, ale ukončovaciu podmienku pre mieru zmeny váh (tolerancia). Samotný učiaci proces vieme vyjadriť nasledovne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.linalg.norm(W_oja - prev_W_oja) > tolerance:\n",
    "    prev_W_oja = W_oja.copy()\n",
    "    for x in Xs:\n",
    "        y = np.dot(x, W_oja)\n",
    "\n",
    "        dW_oja = learning_rate * y * (x - W_oja.T * y).reshape((2, 1))\n",
    "        W_oja += dW_oja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Vizualizácia výsledku\n",
    "\n",
    "Vlastný vektor nájdeme vo váhach ako stĺpec, preto ho najprv pretransformujeme na riadky a zoberieme prvý člen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oja_vec = W_oja.T[0]\n",
    "print(oja_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako vidíme, hodnoty vo vektore sú blízko druhému stĺpcovému vektoru v premennej `eigv`. Vektor aj dataset následne vizualizujeme pomocou `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xs[:, 0], Xs[:, 1])\n",
    "plt.quiver(*[0, 0], *oja_vec, color=['r'], scale=21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sangerovo pravidlo\n",
    "\n",
    "Zovšeobecnený Hebbov algoritmus, ktorý umožňuje trénovanie sietí aj s viacerými výstupmi, sa riadi Sangerovým pravidlom:\n",
    "\n",
    "$\\Delta w_{ij} = \\gamma (y_{i}x_{j} - y_{i}\\sum_{k=1}^{i}w_{kj}y_{k})$\n",
    "\n",
    "a pre vektorovú formu:\n",
    "\n",
    "$\\Delta \\bar{w} = \\gamma (\\bar{y} \\cdot \\bar{x}^{T} - tril(\\bar{y} \\cdot \\bar{y}^{T}) \\cdot \\bar{w})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_sanger = np.random.normal(scale=0.1, size=(2, 2))\n",
    "prev_W_sanger = np.ones((2, 2))\n",
    "\n",
    "learning_rate = 0.1\n",
    "nb_iterations = 2000\n",
    "t = 0.0\n",
    "\n",
    "for i in range(nb_iterations):\n",
    "    prev_W_sanger = W_sanger.copy()\n",
    "    dw = np.zeros((2, 2))\n",
    "    t += 1.0\n",
    "\n",
    "    for j in range(Xs.shape[0]):\n",
    "        Ysj = np.dot(W_sanger, Xs[j]).reshape((2, 1))\n",
    "        QYd = np.tril(np.dot(Ysj, Ysj.T))\n",
    "        dw += np.dot(Ysj, Xs[j].reshape((1, 2))) - np.dot(QYd, W_sanger)\n",
    "\n",
    "    W_sanger += (learning_rate / t) * dw\n",
    "    W_sanger /= np.linalg.norm(W_sanger, axis=1).reshape((2, 1))\n",
    "\n",
    "print(W_sanger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kódy a riešenia nájdete [tu (Ojovo pravidlo)](solutions/lab11-oja-solution.py) a [tu (Sangerovo pravidlo)](solutions/lab11-sanger-solution.py)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
